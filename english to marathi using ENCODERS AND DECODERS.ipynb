{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "database -  http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size= 64  # for training\n",
    "epochs = 100\n",
    "latent_dim = 256   \n",
    "num_samples =10000         # number of samples to train on\n",
    "data_path = 'mar.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the data  \n",
    "input_texts = []                   # all english sentances in an array\n",
    "target_texts = []                  # all marathi sentances in an array\n",
    "input_characters = set()           # gives all the all the english characters\n",
    "target_characters = set()          #  gives all the all the marathi characters\n",
    "\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    # we can see in the data set they are seperated by tab space('/t') and the third one is garbage\n",
    "    input_text,target_text, _ = line.split('\\t')\n",
    "#     print(input_text)\n",
    "    # /n is put only to let us know this the end of the sentances\n",
    "    target_text= '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "                                            \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.', 'Run!', 'Run!', 'Run!', 'Run!']\n",
      "['\\tजा.\\n', '\\tपळ!\\n', '\\tधाव!\\n', '\\tपळा!\\n', '\\tधावा!\\n']\n"
     ]
    }
   ],
   "source": [
    "print(input_texts[:5])\n",
    "print(target_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  10000\n",
      "Number of unique input characters:  70\n",
      "Number of unique input characters:  85\n",
      "Max sequence length of inputs:  19\n",
      "Max sequence length of ouputs:  42\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens =  len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples: ',len(input_texts))\n",
    "print('Number of unique input characters: ',num_encoder_tokens)\n",
    "print('Number of unique input characters: ',num_decoder_tokens)\n",
    "print('Max sequence length of inputs: ',max_encoder_seq_length)\n",
    "print('Max sequence length of ouputs: ',max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot coding the characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into dict\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char,i) for i ,char in enumerate(input_characters)])\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(char,i) for i ,char in enumerate(target_characters)])\n",
    "\n",
    "# target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '$': 3,\n",
       " \"'\": 4,\n",
       " ',': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '0': 8,\n",
       " '1': 9,\n",
       " '2': 10,\n",
       " '3': 11,\n",
       " '4': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 3d arrays filled with zeros \n",
    "\n",
    "# num_pairs(total no. of sentance), max_english_sentence_length, num_english_characters\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts),max_decoder_seq_length,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts),max_decoder_seq_length,num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 19, 70)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[1,2:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "# zips create a tupple of the 2 lists afor every iteration in enumerate\n",
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        # if char is in input token dict it is on hotted\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1\n",
    "    for t,char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t,target_token_index[char]] = 1\n",
    "        if t>0:\n",
    "            # decoder_target_data will be ahead by one timestamp and will not \n",
    "            #include the start character\n",
    "            decoder_target_data[i,t-1,target_token_index[char]] = 1\n",
    "    decoder_input_data[i,t+1:,target_token_index[' ']] = 1\n",
    "    decoder_target_data[i,t:,target_token_index[' ']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 70)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states coz no ouputs for encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# we only want ouput and not other things\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 43s 310ms/step - loss: 1.6029 - accuracy: 0.6391 - val_loss: 1.6684 - val_accuracy: 0.5955\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 31s 245ms/step - loss: 1.1520 - accuracy: 0.7088 - val_loss: 1.1888 - val_accuracy: 0.6877\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 0.9091 - accuracy: 0.7632 - val_loss: 1.0366 - val_accuracy: 0.7246\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.7970 - accuracy: 0.7850 - val_loss: 0.9731 - val_accuracy: 0.7350\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.7305 - accuracy: 0.7992 - val_loss: 0.8891 - val_accuracy: 0.7560\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 38s 304ms/step - loss: 0.6835 - accuracy: 0.8111 - val_loss: 0.8525 - val_accuracy: 0.7668\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.6454 - accuracy: 0.8211 - val_loss: 0.8266 - val_accuracy: 0.7736\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.6060 - accuracy: 0.8319 - val_loss: 0.7974 - val_accuracy: 0.7807\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.5752 - accuracy: 0.8399 - val_loss: 0.7671 - val_accuracy: 0.7897\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 42s 335ms/step - loss: 0.5455 - accuracy: 0.8475 - val_loss: 0.7517 - val_accuracy: 0.7903\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.5190 - accuracy: 0.8546 - val_loss: 0.7378 - val_accuracy: 0.7955\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 49s 395ms/step - loss: 0.4962 - accuracy: 0.8607 - val_loss: 0.7220 - val_accuracy: 0.8023\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.4739 - accuracy: 0.8666 - val_loss: 0.7103 - val_accuracy: 0.8042\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.4537 - accuracy: 0.8717 - val_loss: 0.7013 - val_accuracy: 0.8074\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 36s 285ms/step - loss: 0.4347 - accuracy: 0.8771 - val_loss: 0.6959 - val_accuracy: 0.8101\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.4173 - accuracy: 0.8816 - val_loss: 0.6976 - val_accuracy: 0.8113\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.4002 - accuracy: 0.8860 - val_loss: 0.6982 - val_accuracy: 0.8105\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.3846 - accuracy: 0.8902 - val_loss: 0.6908 - val_accuracy: 0.8124\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 41s 327ms/step - loss: 0.3691 - accuracy: 0.8947 - val_loss: 0.6928 - val_accuracy: 0.8142\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 40s 316ms/step - loss: 0.3545 - accuracy: 0.8983 - val_loss: 0.6948 - val_accuracy: 0.8136\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3410 - accuracy: 0.9019 - val_loss: 0.6955 - val_accuracy: 0.8172\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.3268 - accuracy: 0.9060 - val_loss: 0.7037 - val_accuracy: 0.8157\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.3140 - accuracy: 0.9093 - val_loss: 0.7029 - val_accuracy: 0.8153\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 33s 267ms/step - loss: 0.3010 - accuracy: 0.9131 - val_loss: 0.7064 - val_accuracy: 0.8152\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 31s 249ms/step - loss: 0.2889 - accuracy: 0.9165 - val_loss: 0.7151 - val_accuracy: 0.8153\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.2780 - accuracy: 0.9194 - val_loss: 0.7353 - val_accuracy: 0.8133\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.2662 - accuracy: 0.9230 - val_loss: 0.7295 - val_accuracy: 0.8151\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 36s 292ms/step - loss: 0.2552 - accuracy: 0.9257 - val_loss: 0.7393 - val_accuracy: 0.8148\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.2449 - accuracy: 0.9288 - val_loss: 0.7469 - val_accuracy: 0.8134\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.2351 - accuracy: 0.9319 - val_loss: 0.7626 - val_accuracy: 0.8136\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.2256 - accuracy: 0.9342 - val_loss: 0.7613 - val_accuracy: 0.8137\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 48s 388ms/step - loss: 0.2159 - accuracy: 0.9371 - val_loss: 0.7755 - val_accuracy: 0.8140\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 52s 413ms/step - loss: 0.2065 - accuracy: 0.9393 - val_loss: 0.7937 - val_accuracy: 0.8105\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 49s 392ms/step - loss: 0.1991 - accuracy: 0.9417 - val_loss: 0.7930 - val_accuracy: 0.8122\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 39s 310ms/step - loss: 0.1908 - accuracy: 0.9437 - val_loss: 0.8031 - val_accuracy: 0.8123\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 41s 329ms/step - loss: 0.1828 - accuracy: 0.9462 - val_loss: 0.8155 - val_accuracy: 0.8110\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 42s 335ms/step - loss: 0.1754 - accuracy: 0.9482 - val_loss: 0.8226 - val_accuracy: 0.8112\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.1686 - accuracy: 0.9500 - val_loss: 0.8400 - val_accuracy: 0.8094\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.1615 - accuracy: 0.9519 - val_loss: 0.8412 - val_accuracy: 0.8109\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 0.1555 - accuracy: 0.9540 - val_loss: 0.8590 - val_accuracy: 0.8077\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.1496 - accuracy: 0.9555 - val_loss: 0.8706 - val_accuracy: 0.8081\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.1440 - accuracy: 0.9569 - val_loss: 0.8744 - val_accuracy: 0.8093\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 0.1380 - accuracy: 0.9585 - val_loss: 0.8884 - val_accuracy: 0.8088\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.1324 - accuracy: 0.9603 - val_loss: 0.8976 - val_accuracy: 0.8090\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.1278 - accuracy: 0.9617 - val_loss: 0.9148 - val_accuracy: 0.8077\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1227 - accuracy: 0.9633 - val_loss: 0.9394 - val_accuracy: 0.8060\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.1179 - accuracy: 0.9642 - val_loss: 0.9336 - val_accuracy: 0.8074\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1138 - accuracy: 0.9656 - val_loss: 0.9546 - val_accuracy: 0.8059\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.1100 - accuracy: 0.9666 - val_loss: 0.9519 - val_accuracy: 0.8056\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 30s 235ms/step - loss: 0.1056 - accuracy: 0.9678 - val_loss: 0.9663 - val_accuracy: 0.8070\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 27s 220ms/step - loss: 0.1023 - accuracy: 0.9682 - val_loss: 0.9770 - val_accuracy: 0.8048\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.0983 - accuracy: 0.9695 - val_loss: 0.9892 - val_accuracy: 0.8066\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.0958 - accuracy: 0.9702 - val_loss: 0.9934 - val_accuracy: 0.8068\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.0922 - accuracy: 0.9712 - val_loss: 1.0026 - val_accuracy: 0.8055\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 44s 356ms/step - loss: 0.0894 - accuracy: 0.9724 - val_loss: 1.0122 - val_accuracy: 0.8049\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 35s 276ms/step - loss: 0.0862 - accuracy: 0.9726 - val_loss: 1.0148 - val_accuracy: 0.8059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.0836 - accuracy: 0.9734 - val_loss: 1.0383 - val_accuracy: 0.8051\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 31s 250ms/step - loss: 0.0813 - accuracy: 0.9743 - val_loss: 1.0357 - val_accuracy: 0.8038\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 0.0788 - accuracy: 0.9747 - val_loss: 1.0539 - val_accuracy: 0.8034\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 0.0770 - accuracy: 0.9751 - val_loss: 1.0579 - val_accuracy: 0.8032\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0737 - accuracy: 0.9764 - val_loss: 1.0609 - val_accuracy: 0.8041\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0721 - accuracy: 0.9766 - val_loss: 1.0674 - val_accuracy: 0.8042\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0706 - accuracy: 0.9770 - val_loss: 1.0763 - val_accuracy: 0.8059\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.0688 - accuracy: 0.9774 - val_loss: 1.0886 - val_accuracy: 0.8059\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.0670 - accuracy: 0.9776 - val_loss: 1.0832 - val_accuracy: 0.8047\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0649 - accuracy: 0.9783 - val_loss: 1.0920 - val_accuracy: 0.8058\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.0635 - accuracy: 0.9786 - val_loss: 1.1134 - val_accuracy: 0.8034\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.0622 - accuracy: 0.9791 - val_loss: 1.1088 - val_accuracy: 0.8042\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.0608 - accuracy: 0.9795 - val_loss: 1.1177 - val_accuracy: 0.8053\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.0598 - accuracy: 0.9796 - val_loss: 1.1258 - val_accuracy: 0.8053\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 31s 245ms/step - loss: 0.0583 - accuracy: 0.9800 - val_loss: 1.1248 - val_accuracy: 0.8059\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0571 - accuracy: 0.9805 - val_loss: 1.1288 - val_accuracy: 0.8056\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0559 - accuracy: 0.9804 - val_loss: 1.1426 - val_accuracy: 0.8058\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0553 - accuracy: 0.9807 - val_loss: 1.1417 - val_accuracy: 0.8041\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 1.1546 - val_accuracy: 0.8045\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0534 - accuracy: 0.9813 - val_loss: 1.1567 - val_accuracy: 0.8047\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 1.1564 - val_accuracy: 0.8042\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0509 - accuracy: 0.9819 - val_loss: 1.1598 - val_accuracy: 0.8044\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 1.1723 - val_accuracy: 0.8035\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0498 - accuracy: 0.9820 - val_loss: 1.1688 - val_accuracy: 0.8057\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0490 - accuracy: 0.9821 - val_loss: 1.1713 - val_accuracy: 0.8048\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0486 - accuracy: 0.9823 - val_loss: 1.1809 - val_accuracy: 0.8050\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 31s 247ms/step - loss: 0.0476 - accuracy: 0.9826 - val_loss: 1.1816 - val_accuracy: 0.8044\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.0472 - accuracy: 0.9827 - val_loss: 1.2016 - val_accuracy: 0.8037\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.0462 - accuracy: 0.9828 - val_loss: 1.1902 - val_accuracy: 0.8044\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 0.0459 - accuracy: 0.9829 - val_loss: 1.1927 - val_accuracy: 0.8051\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0452 - accuracy: 0.9831 - val_loss: 1.1985 - val_accuracy: 0.8057\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0445 - accuracy: 0.9832 - val_loss: 1.2023 - val_accuracy: 0.8066\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.0443 - accuracy: 0.9832 - val_loss: 1.2077 - val_accuracy: 0.8048\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0440 - accuracy: 0.9834 - val_loss: 1.2082 - val_accuracy: 0.8067\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0434 - accuracy: 0.9835 - val_loss: 1.2120 - val_accuracy: 0.8061\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.0427 - accuracy: 0.9835 - val_loss: 1.2151 - val_accuracy: 0.8052\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0425 - accuracy: 0.9835 - val_loss: 1.2218 - val_accuracy: 0.8061\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.0419 - accuracy: 0.9839 - val_loss: 1.2288 - val_accuracy: 0.8052\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0417 - accuracy: 0.9838 - val_loss: 1.2295 - val_accuracy: 0.8050\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 30s 240ms/step - loss: 0.0410 - accuracy: 0.9840 - val_loss: 1.2355 - val_accuracy: 0.8043\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.0408 - accuracy: 0.9838 - val_loss: 1.2340 - val_accuracy: 0.8037\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 38s 303ms/step - loss: 0.0406 - accuracy: 0.9841 - val_loss: 1.2346 - val_accuracy: 0.8066\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0405 - accuracy: 0.9841 - val_loss: 1.2515 - val_accuracy: 0.8049\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.0398 - accuracy: 0.9843 - val_loss: 1.2412 - val_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01c027a2e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interferance mode (Sampling)\n",
    "\n",
    "\n",
    "\n",
    "1.    encode input and retrieve initial decoder state\n",
    "2.    run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token.\n",
    "3.    Repeat with the current target token and current states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse tokening\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i,char) for char,i in input_token_index.items()\n",
    ")\n",
    "reverse_target_char_index = dict(\n",
    "    (i,char) for char,i in target_token_index.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentences:  I'm fat.\n",
      "Decoded sentences:  मी जाडी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm ill.\n",
      "Decoded sentences:  मी आजारी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's OK.\n",
      "Decoded sentences:  ठीक आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's me!\n",
      "Decoded sentences:  मी आहे!\n",
      "\n",
      "-\n",
      "Input sentences:  It's me.\n",
      "Decoded sentences:  मी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  Me, too.\n",
      "Decoded sentences:  मी पण.\n",
      "\n",
      "-\n",
      "Input sentences:  Me, too.\n",
      "Decoded sentences:  मी पण.\n",
      "\n",
      "-\n",
      "Input sentences:  Open up.\n",
      "Decoded sentences:  उघडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Open up.\n",
      "Decoded sentences:  उघडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Perfect!\n",
      "Decoded sentences:  परिपूर्ण!\n",
      "\n",
      "-\n",
      "Input sentences:  Show me.\n",
      "Decoded sentences:  मला दाखवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Show me.\n",
      "Decoded sentences:  मला दाखवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Shut up!\n",
      "Decoded sentences:  गप्प!\n",
      "\n",
      "-\n",
      "Input sentences:  Shut up!\n",
      "Decoded sentences:  गप्प!\n",
      "\n",
      "-\n",
      "Input sentences:  Shut up!\n",
      "Decoded sentences:  गप्प!\n",
      "\n",
      "-\n",
      "Input sentences:  Tell me.\n",
      "Decoded sentences:  मला सांगा.\n",
      "\n",
      "-\n",
      "Input sentences:  Tell me.\n",
      "Decoded sentences:  मला सांगा.\n",
      "\n",
      "-\n",
      "Input sentences:  Tom ran.\n",
      "Decoded sentences:  टॉम धावला.\n",
      "\n",
      "-\n",
      "Input sentences:  Tom ran.\n",
      "Decoded sentences:  टॉम धावला.\n",
      "\n",
      "-\n",
      "Input sentences:  Tom won.\n",
      "Decoded sentences:  टॉम जिंकला.\n",
      "\n",
      "-\n",
      "Input sentences:  Wake up!\n",
      "Decoded sentences:  जागा हो!\n",
      "\n",
      "-\n",
      "Input sentences:  Wake up!\n",
      "Decoded sentences:  जागा हो!\n",
      "\n",
      "-\n",
      "Input sentences:  Wake up!\n",
      "Decoded sentences:  जागा हो!\n",
      "\n",
      "-\n",
      "Input sentences:  We care.\n",
      "Decoded sentences:  आम्हाला काळजी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  We care.\n",
      "Decoded sentences:  आम्हाला काळजी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  We know.\n",
      "Decoded sentences:  आम्हाला माहीत आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  We know.\n",
      "Decoded sentences:  आम्हाला माहीत आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  We lost.\n",
      "Decoded sentences:  आपण हरलो.\n",
      "\n",
      "-\n",
      "Input sentences:  We lost.\n",
      "Decoded sentences:  आपण हरलो.\n",
      "\n",
      "-\n",
      "Input sentences:  Welcome.\n",
      "Decoded sentences:  सुस्वागतम!\n",
      "\n",
      "-\n",
      "Input sentences:  Welcome.\n",
      "Decoded sentences:  सुस्वागतम!\n",
      "\n",
      "-\n",
      "Input sentences:  Welcome.\n",
      "Decoded sentences:  सुस्वागतम!\n",
      "\n",
      "-\n",
      "Input sentences:  Welcome.\n",
      "Decoded sentences:  सुस्वागतम!\n",
      "\n",
      "-\n",
      "Input sentences:  Who ate?\n",
      "Decoded sentences:  कोण खालं?\n",
      "\n",
      "-\n",
      "Input sentences:  Who ran?\n",
      "Decoded sentences:  कोण पळालं?\n",
      "\n",
      "-\n",
      "Input sentences:  Who ran?\n",
      "Decoded sentences:  कोण पळालं?\n",
      "\n",
      "-\n",
      "Input sentences:  Who won?\n",
      "Decoded sentences:  कोण जिंकलं?\n",
      "\n",
      "-\n",
      "Input sentences:  Why not?\n",
      "Decoded sentences:  का नाही?\n",
      "\n",
      "-\n",
      "Input sentences:  You won.\n",
      "Decoded sentences:  तुम्ही जिंकलात.\n",
      "\n",
      "-\n",
      "Input sentences:  You won.\n",
      "Decoded sentences:  तुम्ही जिंकलात.\n",
      "\n",
      "-\n",
      "Input sentences:  You won.\n",
      "Decoded sentences:  तुम्ही जिंकलात.\n",
      "\n",
      "-\n",
      "Input sentences:  Back off!\n",
      "Decoded sentences:  हट!\n",
      "\n",
      "-\n",
      "Input sentences:  Be quiet.\n",
      "Decoded sentences:  शांत हो.\n",
      "\n",
      "-\n",
      "Input sentences:  Be quiet.\n",
      "Decoded sentences:  शांत हो.\n",
      "\n",
      "-\n",
      "Input sentences:  Beats me.\n",
      "Decoded sentences:  कोणास ठाऊक.\n",
      "\n",
      "-\n",
      "Input sentences:  Beats me.\n",
      "Decoded sentences:  कोणास ठाऊक.\n",
      "\n",
      "-\n",
      "Input sentences:  Call Tom.\n",
      "Decoded sentences:  टॉमला बोलवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Call Tom.\n",
      "Decoded sentences:  टॉमला बोलवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Call Tom.\n",
      "Decoded sentences:  टॉमला बोलवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Call Tom.\n",
      "Decoded sentences:  टॉमला बोलवा.\n",
      "\n",
      "-\n",
      "Input sentences:  Get down.\n",
      "Decoded sentences:  खाली हो.\n",
      "\n",
      "-\n",
      "Input sentences:  Grab Tom.\n",
      "Decoded sentences:  टॉमला पकड.\n",
      "\n",
      "-\n",
      "Input sentences:  Grab Tom.\n",
      "Decoded sentences:  टॉमला पकड.\n",
      "\n",
      "-\n",
      "Input sentences:  Grab him.\n",
      "Decoded sentences:  पकडा त्याला.\n",
      "\n",
      "-\n",
      "Input sentences:  Have fun.\n",
      "Decoded sentences:  मजा कर.\n",
      "\n",
      "-\n",
      "Input sentences:  He spoke.\n",
      "Decoded sentences:  तो बोलला.\n",
      "\n",
      "-\n",
      "Input sentences:  He spoke.\n",
      "Decoded sentences:  तो बोलला.\n",
      "\n",
      "-\n",
      "Input sentences:  I can go.\n",
      "Decoded sentences:  मी जाऊ शकतो.\n",
      "\n",
      "-\n",
      "Input sentences:  I can go.\n",
      "Decoded sentences:  मी जाऊ शकतो.\n",
      "\n",
      "-\n",
      "Input sentences:  I forgot.\n",
      "Decoded sentences:  मी विसरलो.\n",
      "\n",
      "-\n",
      "Input sentences:  I forgot.\n",
      "Decoded sentences:  मी विसरलो.\n",
      "\n",
      "-\n",
      "Input sentences:  I got it.\n",
      "Decoded sentences:  समजलं.\n",
      "\n",
      "-\n",
      "Input sentences:  I got it.\n",
      "Decoded sentences:  समजलं.\n",
      "\n",
      "-\n",
      "Input sentences:  I looked.\n",
      "Decoded sentences:  मी पाहिलं.\n",
      "\n",
      "-\n",
      "Input sentences:  I looked.\n",
      "Decoded sentences:  मी पाहिलं.\n",
      "\n",
      "-\n",
      "Input sentences:  I phoned.\n",
      "Decoded sentences:  मी फोन केला.\n",
      "\n",
      "-\n",
      "Input sentences:  I prayed.\n",
      "Decoded sentences:  मी प्रार्थना केली.\n",
      "\n",
      "-\n",
      "Input sentences:  I shaved.\n",
      "Decoded sentences:  मी दाढी केली.\n",
      "\n",
      "-\n",
      "Input sentences:  I talked.\n",
      "Decoded sentences:  मी बोलले.\n",
      "\n",
      "-\n",
      "Input sentences:  I talked.\n",
      "Decoded sentences:  मी बोलले.\n",
      "\n",
      "-\n",
      "Input sentences:  I use it.\n",
      "Decoded sentences:  मी ते वापरते.\n",
      "\n",
      "-\n",
      "Input sentences:  I use it.\n",
      "Decoded sentences:  मी ते वापरते.\n",
      "\n",
      "-\n",
      "Input sentences:  I'll try.\n",
      "Decoded sentences:  मी प्रयत्न करेन.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm fine.\n",
      "Decoded sentences:  मी बरी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm fine.\n",
      "Decoded sentences:  मी बरी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm full.\n",
      "Decoded sentences:  माझं पोट भरलंय.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm game.\n",
      "Decoded sentences:  मी तयार आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm late.\n",
      "Decoded sentences:  मला उशीर झाला.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm lazy.\n",
      "Decoded sentences:  मी आळशी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I'm poor.\n",
      "Decoded sentences:  मी गरीब आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  I've won.\n",
      "Decoded sentences:  मी जिंकलेय.\n",
      "\n",
      "-\n",
      "Input sentences:  I've won.\n",
      "Decoded sentences:  मी जिंकलेय.\n",
      "\n",
      "-\n",
      "Input sentences:  It's hot.\n",
      "Decoded sentences:  गरम आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's new.\n",
      "Decoded sentences:  नवीन आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's new.\n",
      "Decoded sentences:  नवीन आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's old.\n",
      "Decoded sentences:  जुनी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's old.\n",
      "Decoded sentences:  जुनी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  It's old.\n",
      "Decoded sentences:  जुनी आहे.\n",
      "\n",
      "-\n",
      "Input sentences:  Kiss Tom.\n",
      "Decoded sentences:  टॉमला किस कर.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave it.\n",
      "Decoded sentences:  ते सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave it.\n",
      "Decoded sentences:  ते सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave it.\n",
      "Decoded sentences:  ते सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave it.\n",
      "Decoded sentences:  ते सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave me.\n",
      "Decoded sentences:  मला सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave me.\n",
      "Decoded sentences:  मला सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Leave me.\n",
      "Decoded sentences:  मला सोडा.\n",
      "\n",
      "-\n",
      "Input sentences:  Look out!\n",
      "Decoded sentences:  सांभाळ!\n",
      "\n",
      "-\n",
      "Input sentences:  Marry me.\n",
      "Decoded sentences:  माझ्याबरोबर लग्न करा.\n",
      "\n",
      "-\n",
      "Input sentences:  Marry me.\n",
      "Decoded sentences:  माझ्याबरोबर लग्न करा.\n",
      "\n",
      "-\n",
      "Input sentences:  She came.\n",
      "Decoded sentences:  त्या आल्या.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100,200):\n",
    "    \n",
    "    # take one sequence (part of the trainign set)\n",
    "    # for tying out decodin\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentences: ',input_texts[seq_index])\n",
    "    print('Decoded sentences: ',decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd09043f088b5ada3303788f898933593b6369d15d1dcd091dac703a7e64d4ef4cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
